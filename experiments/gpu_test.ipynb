{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we test if GPU is working... we will run this notebook on both a server with GPU and on my local machine with only CPU. \n",
    "\n",
    "This notebook is identical to cpu_test and gpu_test_with_flags\n",
    "\n",
    "On the server, we check nvidia-smi to see if it is actually using GPU. \n",
    "\n",
    "Then, we compare runtimes for this notebook on GPU vs CPU to make sure that GPU isn't inadvertently making things slower (e.g. by repeatedly moving data btwn CPU/GPU, etc). Also we should check how much faster GPU makes things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython extension to autoreload imported modules so that any changes will be up to date before running code in this nb\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_training import train_and_evaluate_with_data, create_dataset\n",
    "# from utils.jraph_models import MLPGraphNetwork\n",
    "from utils.jraph_data import print_graph_fts\n",
    "from utils.jraph_vis import plot_predictions\n",
    "from utils.hyperparam_tuning import remove_bad_trials, get_best_trial_config, get_best_trial_workdir\n",
    "import ml_collections\n",
    "import optuna \n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up functions for optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"experiments/tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, study_name, datasets):\n",
    "    \"\"\" Defines the objective function to be optimized over, aka the validation loss of a model.\n",
    "    \n",
    "        Args:\n",
    "            trial: object which characterizes the current run \n",
    "            datasets: dictionary of data. we explicitly pass this in so that we don't have to waste runtime regenerating the same dataset over and over. \n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    # create config \n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Optimizer.\n",
    "    # config.optimizer = \"adam\"\n",
    "    config.optimizer = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    config.learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-2, \n",
    "                                               log=True)\n",
    "    if config.optimizer == \"sgd\":\n",
    "        config.momentum = trial.suggest_float('momentum', 0, 0.999) # upper bound is inclusive, and we want to exclude a momentum of 1 because that would yield no decay \n",
    "\n",
    "    # Data params that are used in training \n",
    "    config.output_steps = 4\n",
    "\n",
    "    # Training hyperparameters.\n",
    "    config.batch_size = 1 # variable currently not used\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 5\n",
    "    config.eval_every_epochs = 5\n",
    "    config.checkpoint_every_epochs = 10\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPBlock'\n",
    "    config.dropout_rate = trial.suggest_float('dropout_rate', 0, 0.6)\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.activation = trial.suggest_categorical(\n",
    "        'activation', [\"relu\", \"elu\", \"leaky_relu\"])\n",
    "    \n",
    "    # choose the hidden layer feature size using powers of 2 \n",
    "    config.edge_features = (\n",
    "        2**trial.suggest_int(\"edge_mlp_1_power\", 1, 3), # range 2 - 8; upper bound is inclusive\n",
    "        2**trial.suggest_int(\"edge_mlp_2_power\", 1, 3), # range 2 - 8\n",
    "    )\n",
    "    config.node_features = (\n",
    "        2**trial.suggest_int(\"node_mlp_1_power\", 1, 8), # range 2 - 512\n",
    "        2**trial.suggest_int(\"node_mlp_2_power\", 1, 8), # range 2 - 512\n",
    "        2) \n",
    "    # note the last feature size will be the number of features that the graph predicts\n",
    "    config.global_features = None\n",
    "\n",
    "    # generate a workdir \n",
    "    # TODO: check if we actually care about referencing this in the future or if we can just create a temp dir \n",
    "    workdir=os.path.join(CHECKPOINT_PATH, study_name, f\"trial_{trial.number}\")\n",
    "\n",
    "    # run training \n",
    "    state, train_metrics, eval_metrics_dict = train_and_evaluate_with_data(config=config, workdir=workdir, datasets=datasets, trial=trial)\n",
    "    \n",
    "    # retrieve and return val loss (MSE)\n",
    "    # print(\"eval_metrics_dict['val'].loss\", eval_metrics_dict['val'].loss)\n",
    "    # print(\"eval_metrics_dict['val'].compute()['loss']\", eval_metrics_dict['val'].compute()['loss'])\n",
    "    # print()\n",
    "    end = datetime.now()\n",
    "\n",
    "    print(f\"objective func took {end - start} to run\")\n",
    "    return eval_metrics_dict['val'].compute()['loss']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_config():\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.n_samples=10_000\n",
    "    config.input_steps=1\n",
    "    config.output_delay=8 # predict 24 hrs into the future \n",
    "    config.output_steps=4\n",
    "    config.timestep_duration=3 # equivalent to 3 hours\n",
    "    # note a 3 hour timestep resolution would be 5*24/3=40\n",
    "    # if the time_resolution is 120, then a sampling frequency of 3 would achieve a 3 hour timestep \n",
    "    config.sample_buffer = -1 * (config.input_steps + config.output_delay + config.output_steps - 1) # negative buffer so that our sample input are continuous (i.e. the first sample would overlap a bit with consecutive samples) \n",
    "        # number of timesteps strictly between the end \n",
    "        # of one full sample and the start of the next sample\n",
    "    config.time_resolution=120 # the number of \n",
    "                # raw data points generated per time unit, equivalent to the \n",
    "                # number of data points generated per 5 days in the simulation\n",
    "    config.init_buffer_samples=100\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=42\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=False\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_study(study_name):\n",
    "    # generate dataset \n",
    "    data_start = datetime.now()\n",
    "    dataset_config = get_data_config()\n",
    "    datasets = create_dataset(dataset_config)\n",
    "    data_end = datetime.now()\n",
    "    print(f\"dataset took {data_end - data_start} time to generate\")\n",
    "    \n",
    "    print_graph_fts(datasets['train']['inputs'][0][0])\n",
    "\n",
    "    # get the objective function that reuses the pre-generated datasets \n",
    "    objective_partial = partial(objective, study_name=study_name, \n",
    "                                datasets=datasets)\n",
    "\n",
    "    # run optimization study\n",
    "    db_path = os.path.join(CHECKPOINT_PATH, study_name, \"optuna_hparam_search.db\")\n",
    "    if not os.path.exists(os.path.join(CHECKPOINT_PATH, study_name)):\n",
    "        os.makedirs(os.path.join(CHECKPOINT_PATH, study_name))\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f'sqlite:///{db_path}', # generates a new db if it doesn't exist\n",
    "        direction='minimize',\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5, \n",
    "            n_warmup_steps=50,\n",
    "            ), \n",
    "        load_if_exists=True, \n",
    "    )\n",
    "    \n",
    "    return study, objective_partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get study\n",
    "study_gpu, objective_partial = prepare_study(study_name=\"hparam_study_gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_gpu.optimize(objective_partial, \n",
    "                n_trials=10-len(study_gpu.trials), \n",
    "                n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = create_dataset(get_data_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    config=get_best_trial_config(study=study_gpu),\n",
    "    workdir=get_best_trial_workdir(study=study_gpu), # for loading checkpoints \n",
    "    plot_ith_rollout_step=0, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    # dataset,\n",
    "    # preds,\n",
    "    # timestep_duration,\n",
    "    # n_rollout_steps,\n",
    "    #  total_steps,\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"val\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    datasets=datasets,\n",
    "    plot_all=False, # if false, only plot the first 100 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_vis = remove_bad_trials(study_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study_vis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['learning_rate', 'dropout_rate'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['learning_rate', 'edge_mlp_1_power'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['learning_rate', 'node_mlp_1_power'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['learning_rate', 'optimizer'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['optimizer', 'activation'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the estimated accuracy surface over hyperparameters:\n",
    "fig = optuna.visualization.plot_contour(study_vis, params=['activation', 'node_mlp_1_power'])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorenzvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
