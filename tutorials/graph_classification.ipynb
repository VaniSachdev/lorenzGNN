{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through [this tutorial](https://graphneural.network/getting-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 11:48:59.248463: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from spektral.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PROTEINS dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 447k/447k [00:00<00:00, 591kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PROTEINS.\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset('PROTEINS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUDataset(n_graphs=1113)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]\n",
    "dataset.filter(lambda g: g.n_nodes < 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the maximum degree of the dataset, so that we know the size of the one-hot vectors\n",
    "max_degree = dataset.map(lambda g: g.a.sum(-1).max(), reduce=max)\n",
    "max_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment our node features with the one-hot-encoded degree\n",
    "from spektral.transforms import Degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply(Degree(int(max_degree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=42, n_node_features=17, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following this paper: https://arxiv.org/abs/1609.02907\n",
    "# so do extra pre-processing of the adjacency matrix\n",
    "\n",
    "from spektral.transforms import GCNFilter\n",
    "dataset.apply(GCNFilter())\n",
    "\n",
    "# every convolutional layer in Spektral has a preprocess(a) method that you can use to transform the adjacency matrix as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from spektral.layers import GCNConv, GlobalSumPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstGNN(Model):\n",
    "\n",
    "    def __init__(self, n_hidden, n_labels):\n",
    "        super().__init__()\n",
    "        self.graph_conv = GCNConv(n_hidden)\n",
    "        self.pool = GlobalSumPool()\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense = Dense(n_labels, 'softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.graph_conv(inputs)\n",
    "        out = self.dropout(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 12:00:59.122595: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = MyFirstGNN(32, dataset.n_labels)\n",
    "model.compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/7p6zt3rd1jz7gwd3wmbkzcw80000gn/T/ipykernel_5376/3333434835.py:2: UserWarning: you are shuffling a 'TUDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(dataset)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.shuffle(dataset)\n",
    "split = int(0.8 * len(dataset))\n",
    "dataset_train, dataset_test = dataset[:split], dataset[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'TUDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 2s 13ms/step - loss: 7.5035\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 6.5851\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 6.0564\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 5.6047\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 4.8682\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 5.5860\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 5.9103\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 5.4632\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 4.1150\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3.6491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6658e6b50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#terating over a dataset in mini-batches is not trivial and we cannot simply use the model.fit() bc we can't resize graphs \n",
    "# user loader!!\n",
    "\n",
    "from spektral.data import BatchLoader\n",
    "loader = BatchLoader(dataset_train, batch_size=32)\n",
    "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = BatchLoader(dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step - loss: 1.1552\n",
      "Test loss: 1.1552215814590454\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)\n",
    "\n",
    "print('Test loss: {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
