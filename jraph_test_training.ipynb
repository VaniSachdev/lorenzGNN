{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython extension to autoreload imported modules so that any changes will be up to date before running code in this nb\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 12:05:45.725556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils.jraph_data import get_lorenz_graph_tuples, print_graph_fts\n",
    "from flax_gnn_example.train import rollout_loss, train_step, train_step_fn #, rollout_loss_batched, \n",
    "from utils.jraph_models import MLPBlock\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test single rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(seed=42):\n",
    "    sample_dataset = get_lorenz_graph_tuples(n_samples=2,\n",
    "                        input_steps=3,\n",
    "                        output_delay=0,\n",
    "                        output_steps=2,\n",
    "                        timestep_duration=1,\n",
    "                        sample_buffer=1,\n",
    "                        time_resolution=100,\n",
    "                        init_buffer_samples=0,\n",
    "                        train_pct=1.0,\n",
    "                        val_pct=0,\n",
    "                        test_pct=0,\n",
    "                        K=36,\n",
    "                        F=8,\n",
    "                        c=10,\n",
    "                        b=10,\n",
    "                        h=1,\n",
    "                        seed=seed,\n",
    "                        normalize=False)\n",
    "    # input_window = sample_dataset['train']['input'][0]\n",
    "    # target_window = sample_dataset['train']['targets'][0]\n",
    "    return sample_dataset \n",
    "\n",
    "sample_dataset = get_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 36\n",
      "Number of edges: 180\n",
      "Node features shape: (36, 2)\n",
      "Edge features shape: (180, 1)\n",
      "Global features shape: (1, 1)\n",
      "Number of nodes: 36\n",
      "Number of edges: 180\n",
      "Node features shape: (36, 2)\n",
      "Edge features shape: (180, 1)\n",
      "Global features shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "sample_input_window = sample_dataset['train']['inputs'][0]\n",
    "sample_target_window = sample_dataset['train']['targets'][0]\n",
    "\n",
    "sample_input_graph = sample_input_window[0]\n",
    "sample_target_graph = sample_target_window[0]\n",
    "\n",
    "sample_input_batch = sample_dataset['train']['inputs']\n",
    "sample_target_batch = sample_dataset['train']['targets']\n",
    "\n",
    "print_graph_fts(sample_input_graph)\n",
    "print_graph_fts(sample_target_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 2)\n",
      "[36]\n",
      "(180, 1)\n",
      "[180]\n",
      "(180,)\n",
      "36\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(sample_input_graph.nodes.shape)\n",
    "print(sample_input_graph.n_node)\n",
    "print(sample_input_graph.edges.shape)\n",
    "print(sample_input_graph.n_edge)\n",
    "print(sample_input_graph.receivers.shape)\n",
    "print(sample_input_graph.n_node[1])\n",
    "print(sample_input_graph.n_node.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up state \n",
    "\n",
    "hidden_layer_features = {'edge': [16, 8], \n",
    "                        'node': [32, 2], 'global': None}\n",
    "model = MLPBlock(edge_features=hidden_layer_features['edge'],\n",
    "                node_features=hidden_layer_features['node'],\n",
    "                global_features=hidden_layer_features['global'])\n",
    "\n",
    "# set up params\n",
    "# init_graphs = test_input_graph\n",
    "rng = jax.random.key(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = jax.jit(model.init)(init_rng, sample_input_window)\n",
    "\n",
    "# set up optimizer (needed for the state even if we aren't training)\n",
    "learning_rate = 0.001  # default learning rate for adam in keras\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# set up state object, which helps us keep track of the model, params, and optimizer\n",
    "state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                        params=params,\n",
    "                                        tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test single rollout \n",
    "avg_loss, pred_nodes = rollout_loss(\n",
    "    state=state, \n",
    "    n_steps=len(sample_target_window),\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window,\n",
    "    rngs=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1143749\n",
      "<class 'list'>\n",
      "2\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "[[8.        2.4399762]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.4399772]\n",
      " [8.        2.4399772]\n",
      " [8.00025   2.4400175]\n",
      " [8.        2.4399822]\n",
      " [8.        2.43998  ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.439976 ]\n",
      " [8.        2.4399762]]\n",
      "[[7.9902935 2.423674 ]\n",
      " [7.9929175 2.42488  ]\n",
      " [7.990142  2.4256337]\n",
      " [7.9900727 2.4255981]\n",
      " [7.9901466 2.4255629]\n",
      " [7.990148  2.425564 ]\n",
      " [7.9901466 2.425565 ]\n",
      " [7.990146  2.4255652]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.425565 ]\n",
      " [7.9901466 2.4255652]\n",
      " [7.990147  2.425565 ]\n",
      " [7.990166  2.4255683]\n",
      " [7.990394  2.4256074]\n",
      " [7.9901447 2.4255714]\n",
      " [7.9901266 2.4255657]\n",
      " [7.9901466 2.4255643]\n",
      " [7.990147  2.4255645]\n",
      " [7.9901466 2.4255648]\n",
      " [7.990146  2.425565 ]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255648]\n",
      " [7.9901466 2.4255643]\n",
      " [7.9901466 2.4255645]\n",
      " [7.9901466 2.4255686]\n",
      " [7.9901466 2.4255662]\n",
      " [7.9901466 2.4255464]\n",
      " [7.9901447 2.42555  ]\n",
      " [7.9900723 2.425378 ]\n",
      " [7.9873753 2.4249082]\n",
      " [7.9207993 2.413727 ]]\n"
     ]
    }
   ],
   "source": [
    "print(avg_loss)\n",
    "print(type(pred_nodes))\n",
    "print(len(pred_nodes))\n",
    "\n",
    "print(type(pred_nodes[0]))\n",
    "print(pred_nodes[0].shape)\n",
    "print(pred_nodes[0])\n",
    "print(pred_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test rollout loss batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, we're getting an issue that i don't know how to immediately fix and batching isn't our top priority right now so i'm going to leave this loose end hanging. TODO later \n",
    "\n",
    "the problem: when we treat a list of GraphsTuples as a jax pytree, for some reason, it treats each attribute of the named tuple as a leaf in the pytree?? so we have n_windows * n_elements in the graphtuple number of leaves. \n",
    "\n",
    "what we'd need to do to fix it is to treat each GraphsTuple as a unique leaf. not sure how to set this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(sample_input_batch)\n",
    "# print_graph_fts(sample_input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(180,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "jax.tree_util.tree_leaves(sample_input_batch)\n",
    "for leaf in jax.tree_util.tree_leaves(sample_input_batch):\n",
    "    print(type(leaf))\n",
    "    print(leaf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_avg_loss, batch_pred_nodes = rollout_loss_batched(state, \n",
    "#                  sample_input_batch,\n",
    "#                  sample_target_batch,\n",
    "#                  None,\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['params'])\n",
      "dict_keys(['MLP_0', 'MLP_1'])\n",
      "dict_keys(['Dense_0', 'Dense_1'])\n",
      "dict_keys(['bias', 'kernel'])\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(16,)\n",
      "(6, 16)\n",
      "(8,)\n",
      "(16, 8)\n",
      "(32,)\n",
      "(19, 32)\n",
      "(2,)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "# check number of params\n",
    "print(type(params))\n",
    "print(params.keys())\n",
    "print(params['params'].keys())\n",
    "print(params['params']['MLP_0'].keys())\n",
    "print(params['params']['MLP_0']['Dense_0'].keys())\n",
    "print(type(params['params']['MLP_0']['Dense_0']['bias']))\n",
    "print(params['params']['MLP_0']['Dense_0']['bias'].shape)\n",
    "print(params['params']['MLP_0']['Dense_0']['kernel'].shape)\n",
    "print(params['params']['MLP_0']['Dense_1']['bias'].shape)\n",
    "print(params['params']['MLP_0']['Dense_1']['kernel'].shape)\n",
    "print(params['params']['MLP_1']['Dense_0']['bias'].shape)\n",
    "print(params['params']['MLP_1']['Dense_0']['kernel'].shape)\n",
    "print(params['params']['MLP_1']['Dense_1']['bias'].shape)\n",
    "print(params['params']['MLP_1']['Dense_1']['kernel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads {'params': {'MLP_0': {'Dense_0': {'bias': Array([-0.05914649,  0.        ,  0.3737321 ,  0.        ,  0.        ,\n",
      "        0.40158406, -0.25132346,  0.        , -0.02082475, -0.23355797,\n",
      "        0.        ,  0.        ,  0.        ,  0.2074176 ,  0.        ,\n",
      "        0.        ], dtype=float32), 'kernel': Array([[ 0.00374286,  0.        ,  0.1065864 ,  0.        ,  0.        ,\n",
      "         0.27213928, -0.5235371 ,  0.        ,  0.08738203, -0.03553124,\n",
      "         0.        ,  0.        ,  0.        , -0.11097553,  0.        ,\n",
      "         0.        ],\n",
      "       [-0.47282547,  0.        ,  2.9876614 ,  0.        ,  0.        ,\n",
      "         3.2103102 , -2.0091085 ,  0.        , -0.16647603, -1.86709   ,\n",
      "         0.        ,  0.        ,  0.        ,  1.6581202 ,  0.        ,\n",
      "         0.        ],\n",
      "       [-0.05858672,  0.        ,  0.37016904,  0.        ,  0.        ,\n",
      "         0.3977499 , -0.24891962,  0.        , -0.02062717, -0.23132828,\n",
      "         0.        ,  0.        ,  0.        ,  0.20543559,  0.        ,\n",
      "         0.        ],\n",
      "       [-0.4728608 ,  0.        ,  2.987691  ,  0.        ,  0.        ,\n",
      "         3.210302  , -2.0090709 ,  0.        , -0.16648443, -1.8670868 ,\n",
      "         0.        ,  0.        ,  0.        ,  1.658104  ,  0.        ,\n",
      "         0.        ],\n",
      "       [-0.05858683,  0.        ,  0.37016916,  0.        ,  0.        ,\n",
      "         0.39774993, -0.24891949,  0.        , -0.02062719, -0.23132826,\n",
      "         0.        ,  0.        ,  0.        ,  0.20543554,  0.        ,\n",
      "         0.        ],\n",
      "       [-0.05914649,  0.        ,  0.37373215,  0.        ,  0.        ,\n",
      "         0.4015839 , -0.2513234 ,  0.        , -0.02082475, -0.23355795,\n",
      "         0.        ,  0.        ,  0.        ,  0.20741756,  0.        ,\n",
      "         0.        ]], dtype=float32)}, 'Dense_1': {'bias': Array([ 0.        ,  0.        ,  0.8846513 , -0.11216369,  1.0222604 ,\n",
      "        0.        , -0.31503713,  0.        ], dtype=float32), 'kernel': Array([[ 0.        ,  0.        ,  3.8929386 , -0.4508161 ,  4.368574  ,\n",
      "         0.        , -1.3859407 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  5.525484  , -0.5837512 ,  6.0300894 ,\n",
      "         0.        , -1.9666919 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  5.3467817 , -0.5917779 ,  5.916812  ,\n",
      "         0.        , -1.9032497 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  3.7541342 , -0.42574307,  4.1854734 ,\n",
      "         0.        , -1.3364247 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  5.6111155 , -0.84388816,  6.88636   ,\n",
      "         0.        , -1.9992466 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  6.194147  , -0.65243715,  6.7538767 ,\n",
      "         0.        , -2.2046018 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  2.6327174 , -0.5577874 ,  3.7227216 ,\n",
      "         0.        , -0.9393899 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ]], dtype=float32)}}, 'MLP_1': {'Dense_0': {'bias': Array([ 0.        ,  0.12312508,  0.08805402,  0.        ,  0.1231331 ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.26873776,\n",
      "        0.12631181,  0.37971264,  0.        ,  0.        , -0.169432  ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.3233586 ,  0.        ,  0.45004493, -0.264166  ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.20650865,\n",
      "        0.        ,  0.        ], dtype=float32), 'kernel': Array([[ 0.        ,  0.98427725,  0.70391476,  0.        ,  0.9843414 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -2.148323  ,\n",
      "         1.0097523 ,  3.0354705 ,  0.        ,  0.        , -1.3544605 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  2.584969  ,  0.        ,  3.597715  , -2.1117759 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  1.650856  ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.12194999,  0.08721364,  0.        ,  0.12195793,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.26617295,\n",
      "         0.12510629,  0.37608868,  0.        ,  0.        , -0.16781496,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.32027245,  0.        ,  0.4457497 , -0.26164478,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.20453775,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.15699822,  0.11227868,  0.        ,  0.15700844,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.34267065,\n",
      "         0.16106164,  0.48417592,  0.        ,  0.        , -0.21604466,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.4123183 ,  0.        ,  0.5738575 , -0.33684108,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.26332155,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.03081686,  0.02203896,  0.        ,  0.03081887,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.06726213,\n",
      "         0.03161446,  0.09503792,  0.        ,  0.        , -0.04240698,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.08093311,  0.        ,  0.11264132, -0.06611785,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.05168686,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  1.312068  ,  0.93833715,  0.        ,  1.3121533 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -2.8637724 ,\n",
      "         1.3460269 ,  4.046363  ,  0.        ,  0.        , -1.8055317 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  3.4458323 ,  0.        ,  4.7958508 , -2.815053  ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  2.200635  ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.18034726,  0.12897697,  0.        ,  0.180359  ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.3936332 ,\n",
      "         0.18501501,  0.55618346,  0.        ,  0.        , -0.24817523,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.4736389 ,  0.        ,  0.65920246, -0.38693666,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.30248323,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.15699777,  0.11227838,  0.        ,  0.157008  ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.34266967,\n",
      "         0.1610612 ,  0.4841746 ,  0.        ,  0.        , -0.21604407,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.41231713,  0.        ,  0.57385594, -0.33684015,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.26332083,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.03081709,  0.02203912,  0.        ,  0.0308191 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.06726262,\n",
      "         0.03161469,  0.09503862,  0.        ,  0.        , -0.04240729,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.0809337 ,  0.        ,  0.11264215, -0.06611834,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.05168724,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  1.3120687 ,  0.9383377 ,  0.        ,  1.312154  ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -2.8637738 ,\n",
      "         1.3460277 ,  4.0463657 ,  0.        ,  0.        , -1.8055332 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  3.4458346 ,  0.        ,  4.795854  , -2.8150551 ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  2.2006364 ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.18034682,  0.12897666,  0.        ,  0.18035859,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.39363217,\n",
      "         0.18501455,  0.556182  ,  0.        ,  0.        , -0.24817464,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.47363773,  0.        ,  0.6592009 , -0.38693574,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.3024825 ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.12312508,  0.08805402,  0.        ,  0.12313311,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.26873776,\n",
      "         0.12631181,  0.37971264,  0.        ,  0.        , -0.16943198,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.3233586 ,  0.        ,  0.4500449 , -0.264166  ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.20650867,\n",
      "         0.        ,  0.        ]], dtype=float32)}, 'Dense_1': {'bias': Array([0.       , 1.4921234], dtype=float32), 'kernel': Array([[ 0.        ,  0.        ],\n",
      "       [ 0.        ,  3.7692904 ],\n",
      "       [ 0.        ,  4.2416706 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        , 11.046227  ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  2.2593417 ],\n",
      "       [ 0.        ,  0.12417376],\n",
      "       [ 0.        ,  6.3472176 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  9.0124035 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  2.9552546 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  1.4330301 ],\n",
      "       [ 0.        ,  3.6945052 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.46870375],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ]], dtype=float32)}}}}\n",
      "> \u001b[0;32m/Users/h.lu/Documents/_code/_research lorenz code/lorenzGNN/flax_gnn_example/train.py\u001b[0m(235)\u001b[0;36mtrain_step_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    233 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    234 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 235 \u001b[0;31m    \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update params in the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    236 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    237 \u001b[0;31m    \u001b[0mmetrics_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_from_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run train step\n",
    "new_state, metrics_update, pred_nodes = train_step_fn(\n",
    "    state=state,\n",
    "    n_steps=len(sample_target_window),\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window,\n",
    "    rngs={'dropout': rng}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try fixing jit batching issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'n_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/h.lu/Documents/_code/_research lorenz code/lorenzGNN/jraph_test_training.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/h.lu/Documents/_code/_research%20lorenz%20code/lorenzGNN/jraph_test_training.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjraph\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/h.lu/Documents/_code/_research%20lorenz%20code/lorenzGNN/jraph_test_training.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mjit(jraph\u001b[39m.\u001b[39;49mbatch)(sample_input_batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/h.lu/Documents/_code/_research%20lorenz%20code/lorenzGNN/jraph_test_training.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m print_graph_fts(batch)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/_code/_research lorenz code/lorenzGNN/lorenzvenv/lib/python3.9/site-packages/jraph/_src/utils.py:477\u001b[0m, in \u001b[0;36mbatch\u001b[0;34m(graphs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch\u001b[39m(graphs: Sequence[gn_graph\u001b[39m.\u001b[39mGraphsTuple]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m gn_graph\u001b[39m.\u001b[39mGraphsTuple:\n\u001b[1;32m    425\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a batched graph given a list of graphs.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[39m  This method will concatenate the ``nodes``, ``edges`` and ``globals``,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m      graph.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m   \u001b[39mreturn\u001b[39;00m _batch(graphs, np_\u001b[39m=\u001b[39;49mjnp)\n",
      "File \u001b[0;32m~/Documents/_code/_research lorenz code/lorenzGNN/lorenzvenv/lib/python3.9/site-packages/jraph/_src/utils.py:490\u001b[0m, in \u001b[0;36m_batch\u001b[0;34m(graphs, np_)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns batched graph given a list of graphs and a numpy-like module.\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m# Calculates offsets for sender and receiver arrays, caused by concatenating\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m# the nodes arrays.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m offsets \u001b[39m=\u001b[39m np_\u001b[39m.\u001b[39mcumsum(\n\u001b[0;32m--> 490\u001b[0m     np_\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m [np_\u001b[39m.\u001b[39msum(g\u001b[39m.\u001b[39mn_node) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]))\n\u001b[1;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_map_concat\u001b[39m(nests):\n\u001b[1;32m    493\u001b[0m   concat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs: np_\u001b[39m.\u001b[39mconcatenate(args)\n",
      "File \u001b[0;32m~/Documents/_code/_research lorenz code/lorenzGNN/lorenzvenv/lib/python3.9/site-packages/jraph/_src/utils.py:490\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns batched graph given a list of graphs and a numpy-like module.\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m# Calculates offsets for sender and receiver arrays, caused by concatenating\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m# the nodes arrays.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m offsets \u001b[39m=\u001b[39m np_\u001b[39m.\u001b[39mcumsum(\n\u001b[0;32m--> 490\u001b[0m     np_\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m [np_\u001b[39m.\u001b[39msum(g\u001b[39m.\u001b[39;49mn_node) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]))\n\u001b[1;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_map_concat\u001b[39m(nests):\n\u001b[1;32m    493\u001b[0m   concat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs: np_\u001b[39m.\u001b[39mconcatenate(args)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'n_node'"
     ]
    }
   ],
   "source": [
    "# import jraph\n",
    "# batch = jax.jit(jraph.batch)(sample_input_batch)\n",
    "# print_graph_fts(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flax_gnn_example.train import unbatch_i\n",
    "# first_graph = jax.jit(unbatch_i)(sample_input_window, 0)\n",
    "# first_window = jraph.unbatch(sample_input_window)\n",
    "def func_with_list(l):\n",
    "   res = 0\n",
    "   for i in l:\n",
    "      res += i\n",
    "   return res\n",
    "\n",
    "jax.jit(func_with_list)([1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorenzvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
