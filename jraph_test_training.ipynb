{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ipython extension to autoreload imported modules so that any changes will be up to date before running code in this nb\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_data import get_lorenz_graph_tuples, print_graph_fts\n",
    "from flax_gnn_example.train import rollout_loss, train_step, train_step_fn #, rollout_loss_batched, \n",
    "from utils.jraph_models import MLPBlock\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test single rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(seed=42):\n",
    "    sample_dataset = get_lorenz_graph_tuples(n_samples=2,\n",
    "                        input_steps=3,\n",
    "                        output_delay=0,\n",
    "                        output_steps=2,\n",
    "                        timestep_duration=1,\n",
    "                        sample_buffer=1,\n",
    "                        time_resolution=100,\n",
    "                        init_buffer_samples=0,\n",
    "                        train_pct=1.0,\n",
    "                        val_pct=0,\n",
    "                        test_pct=0,\n",
    "                        K=36,\n",
    "                        F=8,\n",
    "                        c=10,\n",
    "                        b=10,\n",
    "                        h=1,\n",
    "                        seed=seed,\n",
    "                        normalize=False)\n",
    "    # input_window = sample_dataset['train']['input'][0]\n",
    "    # target_window = sample_dataset['train']['targets'][0]\n",
    "    return sample_dataset \n",
    "\n",
    "sample_dataset = get_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 36\n",
      "Number of edges: 180\n",
      "Node features shape: (36, 2)\n",
      "Edge features shape: (180, 1)\n",
      "Global features shape: (1, 1)\n",
      "Number of nodes: 36\n",
      "Number of edges: 180\n",
      "Node features shape: (36, 2)\n",
      "Edge features shape: (180, 1)\n",
      "Global features shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "sample_input_window = sample_dataset['train']['inputs'][0]\n",
    "sample_target_window = sample_dataset['train']['targets'][0]\n",
    "\n",
    "sample_input_graph = sample_input_window[0]\n",
    "sample_target_graph = sample_target_window[0]\n",
    "\n",
    "sample_input_batch = sample_dataset['train']['inputs']\n",
    "sample_target_batch = sample_dataset['train']['targets']\n",
    "\n",
    "print_graph_fts(sample_input_graph)\n",
    "print_graph_fts(sample_target_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 2)\n",
      "[36]\n",
      "(180, 1)\n",
      "[180]\n",
      "(180,)\n",
      "36\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(sample_input_graph.nodes.shape)\n",
    "print(sample_input_graph.n_node)\n",
    "print(sample_input_graph.edges.shape)\n",
    "print(sample_input_graph.n_edge)\n",
    "print(sample_input_graph.receivers.shape)\n",
    "print(sample_input_graph.n_node[1])\n",
    "print(sample_input_graph.n_node.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up state \n",
    "\n",
    "hidden_layer_features = {'edge': [16, 8], \n",
    "                        'node': [32, 2], 'global': None}\n",
    "model = MLPBlock(edge_features=hidden_layer_features['edge'],\n",
    "                node_features=hidden_layer_features['node'],\n",
    "                global_features=hidden_layer_features['global'])\n",
    "\n",
    "# set up params\n",
    "# init_graphs = test_input_graph\n",
    "rng = jax.random.key(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = jax.jit(model.init)(init_rng, sample_input_window)\n",
    "\n",
    "# set up optimizer (needed for the state even if we aren't training)\n",
    "learning_rate = 0.001  # default learning rate for adam in keras\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# set up state object, which helps us keep track of the model, params, and optimizer\n",
    "state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                        params=params,\n",
    "                                        tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test single rollout \n",
    "avg_loss, pred_nodes = rollout_loss(\n",
    "    state=state, \n",
    "    n_steps=len(sample_target_window),\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window,\n",
    "    rngs=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023148963\n",
      "<class 'list'>\n",
      "2\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(36,)\n",
      "[8.      8.      8.      8.      8.      8.      8.      8.      8.\n",
      " 8.      8.      8.      8.      8.      8.      8.      8.00025 8.\n",
      " 8.      8.      8.      8.      8.      8.      8.      8.      8.\n",
      " 8.      8.      8.      8.      8.      8.      8.      8.      8.     ]\n",
      "[7.9902935 7.9929175 7.990142  7.9900727 7.9901466 7.990148  7.9901466\n",
      " 7.990146  7.9901466 7.9901466 7.9901466 7.9901466 7.9901466 7.9901466\n",
      " 7.990147  7.990166  7.990394  7.9901447 7.9901266 7.9901466 7.990147\n",
      " 7.9901466 7.990146  7.9901466 7.9901466 7.9901466 7.9901466 7.9901466\n",
      " 7.9901466 7.9901466 7.9901466 7.9901466 7.9901447 7.9900723 7.9873753\n",
      " 7.9207993]\n"
     ]
    }
   ],
   "source": [
    "print(avg_loss)\n",
    "print(type(pred_nodes))\n",
    "print(len(pred_nodes))\n",
    "\n",
    "print(type(pred_nodes[0]))\n",
    "print(pred_nodes[0].shape)\n",
    "print(pred_nodes[0])\n",
    "print(pred_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test rollout loss batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, we're getting an issue that i don't know how to immediately fix and batching isn't our top priority right now so i'm going to leave this loose end hanging. TODO later \n",
    "\n",
    "the problem: when we treat a list of GraphsTuples as a jax pytree, for some reason, it treats each attribute of the named tuple as a leaf in the pytree?? so we have n_windows * n_elements in the graphtuple number of leaves. \n",
    "\n",
    "what we'd need to do to fix it is to treat each GraphsTuple as a unique leaf. not sure how to set this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 3\n",
      "Number of nodes: [36 36 36]\n",
      "Number of edges: [180 180 180]\n",
      "Node features (total) shape: (108, 2)\n",
      "Edge features (total) shape: (540, 1)\n",
      "Global features shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "type(sample_input_batch)\n",
    "print_graph_fts(sample_input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(108, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(108, 2)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(540,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3, 1)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "jax.tree_util.tree_leaves(sample_input_batch)\n",
    "for leaf in jax.tree_util.tree_leaves(sample_input_batch):\n",
    "    print(type(leaf))\n",
    "    print(leaf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_avg_loss, batch_pred_nodes = rollout_loss_batched(state, \n",
    "#                  sample_input_batch,\n",
    "#                  sample_target_batch,\n",
    "#                  None,\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['params'])\n",
      "dict_keys(['MLP_0', 'MLP_1'])\n",
      "dict_keys(['Dense_0', 'Dense_1'])\n",
      "dict_keys(['bias', 'kernel'])\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(16,)\n",
      "(6, 16)\n",
      "(8,)\n",
      "(16, 8)\n",
      "(32,)\n",
      "(19, 32)\n",
      "(2,)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "# check number of params\n",
    "print(type(params))\n",
    "print(params.keys())\n",
    "print(params['params'].keys())\n",
    "print(params['params']['MLP_0'].keys())\n",
    "print(params['params']['MLP_0']['Dense_0'].keys())\n",
    "print(type(params['params']['MLP_0']['Dense_0']['bias']))\n",
    "print(params['params']['MLP_0']['Dense_0']['bias'].shape)\n",
    "print(params['params']['MLP_0']['Dense_0']['kernel'].shape)\n",
    "print(params['params']['MLP_0']['Dense_1']['bias'].shape)\n",
    "print(params['params']['MLP_0']['Dense_1']['kernel'].shape)\n",
    "print(params['params']['MLP_1']['Dense_0']['bias'].shape)\n",
    "print(params['params']['MLP_1']['Dense_0']['kernel'].shape)\n",
    "print(params['params']['MLP_1']['Dense_1']['bias'].shape)\n",
    "print(params['params']['MLP_1']['Dense_1']['kernel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/h.lu/Documents/_code/_research lorenz code/lorenzGNN/flax_gnn_example/train.py\u001b[0m(233)\u001b[0;36mtrain_step_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    231 \u001b[0;31m    \u001b[0;31m# print('grads', grads['params']['MLP_1']['Dense_1']['kernel'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    232 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 233 \u001b[0;31m    \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update params in the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    234 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    235 \u001b[0;31m    \u001b[0mmetrics_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_from_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# run train step\n",
    "new_state, metrics_update, pred_nodes = train_step_fn(\n",
    "    state=state,\n",
    "    n_steps=len(sample_target_window),\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window,\n",
    "    rngs={'dropout': rng}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 6\n",
      "Number of nodes: [36 36 36 36 36 36]\n",
      "Number of edges: [180 180 180 180 180 180]\n",
      "Node features (total) shape: (216, 2)\n",
      "Edge features (total) shape: (1080, 1)\n",
      "Global features shape: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "import jraph\n",
    "batch = jax.jit(jraph.batch)(sample_input_batch)\n",
    "print_graph_fts(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try fixing jit issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flax_gnn_example.train import unbatch_i\n",
    "# first_graph = jax.jit(unbatch_i)(sample_input_window, 0)\n",
    "# first_window = jraph.unbatch(sample_input_window)\n",
    "def func_with_list(l):\n",
    "   res = 0\n",
    "   for i in l:\n",
    "      res += i\n",
    "   return res\n",
    "\n",
    "jax.jit(func_with_list)([1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorenzvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
